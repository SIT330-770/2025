---
type: lecture
week: Week 7
date: 2025-04-14T10:00:00
title: Transformers and Pretrained LMs
tldr: "Transformers and Pretrained LMs."
hide_from_announcments: true
thumbnail: /static_files/presentations/transformers.jpg
links: 
    - url: /static_files/presentations/Week_7_-_Transformers_and_Pretrained_LMs.pdf
      name: slides
    - url: /static_files/presentations/Week_7_-_Transformers_and_Pretrained_LMs_6up.pdf
      name: slides 6up
---
**Video recordings (2 Hours, 05 Minutes and 14 Seconds):**
- Transformers: Attention Is All You Need! (1 Hour, 10 Minutes and 29 Seconds)
    - [Introduction to Transformers (16:23)](https://youtu.be/KCqihbmWeao)
    - [Self-Attention Mechanism (18:10)](https://youtu.be/qEBFfTywJNg)
    - [The Encoder Transformer Block (10:27)](https://youtu.be/iFD27h617jo)
    - [The Input: Embeddings for Tokens (8:03)](https://youtu.be/DZuZFPH5lbo)
    - [The Input: Embeddings for Positions (11:15)](https://youtu.be/dRQ8cDMbq9E)
    - [The Task Specific Head (6:11)](https://youtu.be/Ek6W2Wd7Ty4)
- Pre-trained LMs (54 Minutes and 45 Seconds)
    - [BERT: Bidirectional Encoder Representations from Transformers (13:34)](https://youtu.be/7QRpWx9UhWo)
    - [BERT pre-training (13:39)](https://youtu.be/CJsVqr5uWvc)
    - [BERT fine-tuning (9:16)](https://youtu.be/yI6CfB_CceY)
    - [BERT Performance (6:11)](https://youtu.be/t46q_h3IC6E)
    - [Other Models Based on Transformers (6:43)](https://youtu.be/DxpJWvW7Mfw)
    - [HuggingFace (5:22)](https://youtu.be/4gLahjgBcqg)
