---
type: lecture
week: Week 7
date: 2025-04-14T10:00:00
title: Transformers and Pretrained LMs
tldr: "Transformers and Pretrained LMs."
hide_from_announcments: true
thumbnail: /static_files/presentations/transformers.jpg
links: 
    - url: /static_files/presentations/Week_7_-_Transformers_and_Pretrained_LMs.pdf
      name: slides
    - url: /static_files/presentations/Week_7_-_Transformers_and_Pretrained_LMs_6up.pdf
      name: slides 6up
---
**Video recordings (2 Hours, 05 Minutes and 12 Seconds):**
- Transformers: Attention Is All You Need! (1 Hour, 10 Minutes and 27 Seconds)
    - [Introduction to Transformers (16:21)](https://youtu.be/ptPKcN2mrEY)
    - [Self-Attention Mechanism (18:10)](https://youtu.be/4JIh1-GE0qw)
    - [The Encoder Transformer Block (10:27)](https://youtu.be/dVw5_-Z9hWY)
    - [The Input: Embeddings for Tokens (8:03)](https://youtu.be/6qK2DZTnAxQ)
    - [The Input: Embeddings for Positions (11:15)](https://youtu.be/a9Id9iv5vE8)
    - [The Task Specific Head (6:11)](https://youtu.be/BeX9SEnQgI8)
- Pre-trained LMs (54 Minutes and 45 Seconds)
    - [BERT: Bidirectional Encoder Representations from Transformers (13:34)](https://youtu.be/zdN_Y8pB04k)
    - [BERT pre-training (13:39)](https://youtu.be/b0uPATLAprY)
    - [BERT fine-tuning (9:16)](https://youtu.be/OVdiS2fgyz4)
    - [BERT Performance (6:11)](https://youtu.be/53rS-ha2I7k)
    - [Other Models Based on Transformers (6:43)](https://youtu.be/5wGgbJEvszc)
    - [HuggingFace (5:22)](https://youtu.be/8F8Ov3MUs8Y)
